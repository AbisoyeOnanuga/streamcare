import os
import replicate
from dotenv import load_dotenv
from utils import log_performance

# Load environment variables from .env file
load_dotenv()

# Retrieve the API token from the environment variable
REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')

# Initialize the Replicate model with the API key
client = replicate.Client(api_token=REPLICATE_API_TOKEN)

model_name = "snowflake/snowflake-arctic-instruct"

# Function to process each synthetic case
def process_synthetic_cases(synthetic_cases, model_name, test_type):
    global test_count
    for case in synthetic_cases:
        test_count += 1  # Increment test count for each case
        input_data = {
            'medications': case['medications'],
            'side_effects': case['side_effects'],
            'medical_condition': case['medical_condition']
        }
        input_prompt = (
            f"As an AI trained in pharmacology, analyze the potential drug-related causes of side effects and the "
            f"impact of the patient's medical condition on their treatment. Medications: {case['medications']}. Reported "
            f"side effects: {case['side_effects']}. Medical condition: {case['medical_condition']}. Provide detailed insights that "
            f"can aid healthcare professionals in making informed treatment decisions."
        )
        # Initialize an empty list to store model outputs for the current case
        model_outputs = []
        relevant_information_generated = False
        
        try:
            # Call the model and collect the output
            for event in client.stream(model_name, input={'prompt': input_prompt, 'temperature': 0.2}):
                if hasattr(event, 'data') and event.data.strip():
                    # Check for and remove any trailing empty dictionary representations
                    cleaned_output = event.data.strip().rstrip('{}')
                    # Append non-empty model output to the list
                    model_outputs.append(cleaned_output)
                    print(cleaned_output)  # Print each part of the model output as it is streamed
                    relevant_information_generated = True
        except Exception as e:
            print(f"An error occurred while streaming: {e}")
            model_outputs.append(f"An error occurred: {e}")

        if not relevant_information_generated:
            # Log only once if no relevant information is generated
            print("No relevant information generated by the model.")
            model_outputs.append("No relevant information generated by the model.")

        # Log the performance for the current case
        log_performance(test_type, model_name, input_data, model_outputs, test_count)

# Function to interact with the user and collect their diagnosis and treatment plan
def user_diagnosis_and_treatment():
    user_diagnosis = input("Enter your diagnosis: ")
    user_treatment_plan = input("Enter your suggested treatment plan: ")
    return user_diagnosis, user_treatment_plan

# Function to provide AI feedback on the user's input
def ai_feedback_on_user_input(user_diagnosis, user_treatment_plan, scenario):
    # Construct the prompt for AI feedback
    ai_feedback_prompt = (
        f"User Diagnosis: {user_diagnosis}\n"
        f"User Treatment Plan: {user_treatment_plan}\n"
        f"Please provide feedback and additional insights based on the user's input and the following patient information:\n"
        f"Medications: {scenario['medications']}\n"
        f"Reported Side Effects: {scenario['side_effects']}\n"
        f"Medical Condition: {scenario['medical_condition']}\n"
    )

    # Initialize an empty list to store AI feedback
    ai_feedback_outputs = []

    try:
        # Call the model and collect the output
        for event in client.stream(model_name, input={'prompt': ai_feedback_prompt, 'temperature': 0.2}):
            if hasattr(event, 'data') and event.data.strip():
                # Append non-empty model output to the list
                ai_feedback_outputs.append(event.data.strip().rstrip('{}'))
    except Exception as e:
        print(f"An error occurred while streaming: {e}")
        ai_feedback_outputs.append(f"An error occurred: {e}")

def run_user_interaction(medications, side_effects, medical_condition, model_name):
    test_type = 'User'
    user_input_prompt = {
        'prompt': (
            f"As an AI trained in pharmacology, analyze the potential drug-related causes of side effects and the "
            f"impact of the patient's medical condition on their treatment. Medications: {medications}. Reported "
            f"side effects: {side_effects}. Medical condition: {medical_condition}. Provide detailed insights that "
            f"can aid healthcare professionals in making informed treatment decisions."
        ),
        'temperature': 0.2
    }
    model_outputs = []
    relevant_information_generated = False

    # Collecting model outputs
    for event in client.stream(model_name, input=user_input_prompt):
        if hasattr(event, 'data'):
            model_output = event.data
            if model_output.strip():
                # Check for and remove any trailing empty dictionary representations
                cleaned_output = model_output.strip().rstrip('{}')
                model_outputs.append(cleaned_output)  # Append to the list
                # Instead of print, you would return or use Streamlit to display
                relevant_information_generated = True

    if not relevant_information_generated:
        # Instead of print, you would return or use Streamlit to display
        model_outputs.append("No relevant information generated by the model.")

    # Log the performance after collecting all outputs
    # Assuming test_count is managed elsewhere or refactored to work within Streamlit
    test_count = 0  # Placeholder for actual test count management
    log_performance(test_type, model_name, {'medications': medications, 'side_effects': side_effects, 'medical_condition': medical_condition}, model_outputs, test_count)
    
    return model_outputs
