import os
import replicate
from dotenv import load_dotenv
from utils import log_performance

# Load environment variables from .env file
load_dotenv()

# Retrieve the API token from the environment variable
REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')

# Initialize the Replicate model with the API key
client = replicate.Client(api_token=REPLICATE_API_TOKEN)

model_name = "snowflake/snowflake-arctic-instruct"

def run_user_interaction(medications, side_effects, medical_condition, model_name):
    global test_count
    test_type = 'User'
    user_input_prompt = {
        'prompt': (
            f"As an AI trained in pharmacology, analyze the potential drug-related causes of side effects and the "
            f"impact of the patient's medical condition on their treatment. Medications: {medications}. Reported "
            f"side effects: {side_effects}. Medical condition: {medical_condition}. Provide detailed insights that "
            f"can aid healthcare professionals in making informed treatment decisions."
        ),
        'temperature': 0.2
    }
    model_outputs = []
    relevant_information_generated = False

    # Collecting model outputs
    for event in client.stream(model_name, input=user_input_prompt):
        if hasattr(event, 'data'):
            model_output = event.data
            if model_output.strip():
                # Check for and remove any trailing empty dictionary representations
                cleaned_output = model_output.strip().rstrip('{}')
                model_outputs.append(cleaned_output)  # Append to the list
                full_ai_response = "\n".join(model_outputs)
                # Instead of print, you would return or use Streamlit to display
                relevant_information_generated = True

    if not relevant_information_generated:
        # Instead of print, you would return or use Streamlit to display
        model_outputs.append("No relevant information generated by the model.")

    # Log the performance after collecting all outputs
    # Assuming test_count is managed elsewhere or refactored to work within Streamlit
    test_count = 0  # initialize test count management
    test_count += 1
    log_performance(test_type, model_name, {'medications': medications, 'side_effects': side_effects, 'medical_condition': medical_condition}, full_ai_response, test_count)
    
    return model_outputs
